---
title: "lab4markdown"
output: 
  html_document:
    toc: yes
    toc_float: yes
    theme: spacelab
    highlight: pygments
date: "2022-09-16"
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Task 1

```{r}
getwd()
```
## Task 2

```{r}
spruce = read.csv("SPRUCE.csv")
tail(spruce)
```

## Task 3

```{r}
library(s20x)
trendscatter(spruce$Height~spruce$BHDiameter,f=.5)
spruce.lm=with(spruce,lm(Height~BHDiameter)) 
height.res=residuals(spruce.lm)
height.fit=fitted(spruce.lm)

plot(height.res~height.fit)

trendscatter(height.res~height.fit)

plot(spruce.lm, which =1)
```
We can see in both the original trendscatter plot, and the residuals vs fitted plot that the data has a distinct curve. 

```{r}
normcheck(spruce.lm,shapiro.wilk = TRUE)
```


The P value here is .29. In this specific case, our null hypothesis is that the data we have follows a normal distribution with approximately equal portions falling to the left and right of the mean. Our P value of .29 is relatively high, and does not give us sufficient reason to reject the null hypothesis. In this case that is desirable, as a sample set that is not normally distributed is harder to draw useful or valid conclusions from.
Based on the plots that we have seen, it's clear that a straight line does not fit the data well and we will have to use a more complex model.

## Task 4

```{r}
quad.lm=lm(Height~BHDiameter + I(BHDiameter^2),data=spruce)
plot(spruce$Height~spruce$BHDiameter)
coef(quad.lm)
myplot=function(x){
 0.86089580 +1.46959217*x  -0.02745726*x^2
}
curve(myplot, lwd=2, col="steelblue",add=TRUE)

quad.res=residuals(quad.lm)
quad.fit=fitted(quad.lm)

plot(quad.res~quad.fit)

normcheck(quad.lm,shapiro.wilk = TRUE)
```
For this normality test we end up with a P value of .684. This value also does not allow us to reject the null hypothesis outright, so we will assume that this also follows a normal distribution. 


## Task 5


```{r}
summary(quad.lm)

```
The Beta values are our estimated coefficients, so $\hat{B_0}$ is 0.860896, $\hat{B_1}$ is  1.469592, and $\hat{B_2}$ is -0.027457.


Interval estimates:


```{r}
ciReg(quad.lm)

```

Equation of the fitted line is: $y = -0.027457x^2 + 1.469592x + 0.860896$


Values predicted using the new model:
```{r}
predict(quad.lm, data.frame(BHDiameter=c(15,18,20)))

```
Values predicted using the old model for BHDiameter = 15, 18, and 20 cm are respectively 16.36895 17.81338 18.77632.

```{r}
summary(quad.lm)$r.squared
summary(spruce.lm)$r.squared
```
In general, multiple R squared tells us how much of the variation of the variable we are interested in can be explained by the model where values closer to 1 are better. The quadratic model is closer to 1, so it is therefore better. 

```{r}
summary(quad.lm)$adj.r.squared
summary(spruce.lm)$adj.r.squared
```
The adjusted R squared is similar, but it is adjusted for the number of variables are using. Since one model is quadratic and the other is linear, we obviously have one more variable in the quadratic model. The higher adjusted R squared value confirms that the extra variable is helping us create a better model.

```{r}
anova(spruce.lm,quad.lm)

anova(quad.lm)
anova(spruce.lm)
```
Here we see a summary of the two models. The easiest value I found to interpret here was the residual sum of squares. This is the measure of how our data differs from the predictions of the model, and since the value for the quadractic model is lower, quite a bit lower in fact, we can conclude that it is a much better model.


```{r}
height.qfit=fitted(quad.lm)

RSS=with(spruce, sum((Height-height.qfit)^2))
RSS
MSS = with(spruce, sum((height.qfit-mean(Height))^2))
MSS
TSS = with(spruce, sum((Height-mean(Height))^2))
TSS

MSS/TSS
```


## Task 6


```{r}
cooks20x(quad.lm)

#Now remove the 24th datum and reanalyze data

quad2.lm=lm(Height~BHDiameter + I(BHDiameter^2) , data=spruce[-24,])
summary(quad2.lm)
summary(quad.lm)
```

Cook's distance is primarily a tool for finding data that is negatively affecting our model. It tells us how much our regression analysis would change if we removed a data point. If the model is a good one, these data points will likely be outliers or errors. Data point 24 has the highest Cook's distance, so we remove that and try a new quadractic model. 

To compare our two models we can just compare the multiple R squared values. For the model with the data point removed, we have a value of 0.8159. For the previous model the value is of course still 0.7741. 0.8159 is higher, so the adjusted model is therefore better.

## Task 7

$l_1: y - B_0 + B_1x $
$l_2: y - B_0 + d + (B_1 + B_2)x $
$y_k = B_0 + B_1x_k = B_0 + d + (B_1 + B_2)x_k$
$y_k = B_0 + B_1x_k = B_0 + d + B_1x_k + B_2x_k$
$0 = d + B_2x_k$
$d = -B_2x_k$
$l_2: y - B_0 + -B_2x_k + B_1x + B_2x $
$l_2: y - B_0 + B_1x + B_2x - B_2x_k$
$l_2: y - B_0 + B_1x + B_2(x - x_k)$
$l_2: y - B_0 + B_1x + B_2(x - x_k)I(x>x_k)$

Where I = 1 if $x>x_k$ and 0 otherwise.
```{r}
sp2.df=within(spruce, X<-(BHDiameter-18)*(BHDiameter>18)) # this makes a new variable and places it within the same df
sp2.df

lmp=lm(Height~BHDiameter + X,data=sp2.df)
tmp=summary(lmp)
names(tmp)

myf = function(x,coef){
  coef[1]+coef[2]*(x) + coef[3]*(x-18)*(x-18>0)
}
plot(spruce,main="Piecewise regression")
myf(0, coef=tmp$coefficients[,"Estimate"])
curve(myf(x,coef=tmp$coefficients[,"Estimate"] ),add=TRUE, lwd=2,col="Blue")
abline(v=18)
text(18,16,paste("R sq.=",round(tmp$r.squared,4) ))
```

## Task 8

```{r}
library(MATH4753FALLbill7368)
with(spruce, scatterhist(BHDiameter,Height, xlab="BHDiameter", ylab = "Height"))
```
This function takes two vectors and produces a scatterplot and histogram from them.
